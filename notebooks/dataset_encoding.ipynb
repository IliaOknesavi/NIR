{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обработка датасетов с различными методами кодирования\n",
    "Этот ноутбук обрабатывает датасеты из `datasets/original/` используя три метода кодирования:\n",
    "- One-Hot Encoding\n",
    "- Label Encoding\n",
    "- Frequency Encoding"
   ],
   "id": "40abc620d75f6c67"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T17:22:41.385964Z",
     "start_time": "2025-12-02T17:22:41.382510Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import os\n",
    "import ast\n",
    "from pathlib import Path"
   ],
   "id": "23565153f8b60ff6",
   "outputs": [],
   "execution_count": 212
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Загрузка реестра датасетов"
   ],
   "id": "a8de4c9ec204268a"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T17:22:41.407084Z",
     "start_time": "2025-12-02T17:22:41.401948Z"
    }
   },
   "source": [
    "def load_datasets_registry(registry_path='../datasets/datasets_registry.csv'):\n",
    "    \"\"\"\n",
    "    Загружает реестр датасетов и преобразует его в список структур.\n",
    "\n",
    "    Args:\n",
    "        registry_path: путь к файлу datasets_registry.csv\n",
    "\n",
    "    Returns:\n",
    "        list: список словарей с информацией о датасетах\n",
    "    \"\"\"\n",
    "    # Читаем CSV с учетом многострочных значений\n",
    "    df = pd.read_csv(registry_path, skipinitialspace=True)\n",
    "\n",
    "    datasets_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Проверяем, является ли значение в cat_col пустым (NaN)\n",
    "        if pd.isna(row['cat_col']):\n",
    "            cat_cols_list = []\n",
    "        else:\n",
    "            # Обрабатываем cat_col - он может быть многострочным\n",
    "            cat_col_str = str(row['cat_col']).replace('\\n', '').replace('\\r', '').strip()\n",
    "            try:\n",
    "                # Проверяем, не пустая ли строка после очистки\n",
    "                if cat_col_str:\n",
    "                    cat_cols_list = ast.literal_eval(cat_col_str)\n",
    "                else:\n",
    "                    cat_cols_list = []\n",
    "            except (ValueError, SyntaxError):\n",
    "                # Если строка все равно не парсится, считаем ее пустой\n",
    "                cat_cols_list = []\n",
    "\n",
    "        dataset_info = {\n",
    "            'dataset_name': row['dataset_name'].strip(),\n",
    "            'dataset_path': row['dataset_path'].strip(),\n",
    "            'dataset_csv': row['dataset_csv'].strip(),\n",
    "            'target': row['target'].strip(),\n",
    "            'cat_cols': cat_cols_list\n",
    "        }\n",
    "        datasets_list.append(dataset_info)\n",
    "\n",
    "    return datasets_list\n"
   ],
   "id": "49898b880d3041e3",
   "outputs": [],
   "execution_count": 213
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T17:22:41.418015Z",
     "start_time": "2025-12-02T17:22:41.412662Z"
    }
   },
   "source": [
    "# Загружаем реестр датасетов\n",
    "# Указываем корректный путь к файлу относительно расположения ноутбука\n",
    "datasets = load_datasets_registry(registry_path='../datasets/datasets_registry.csv')\n",
    "print(f\"Загружено {len(datasets)} датасет(ов)\")\n",
    "print(\"\\nПример структуры:\")\n",
    "print(datasets[0])\n"
   ],
   "id": "f1bc2bc9baeefbfd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружено 2 датасет(ов)\n",
      "\n",
      "Пример структуры:\n",
      "{'dataset_name': 'adult', 'dataset_path': 'datasets/original/adult.csv', 'dataset_csv': 'datasets/adult/data.csv', 'target': 'class', 'cat_cols': ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country', 'class']}\n"
     ]
    }
   ],
   "execution_count": 214
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Функция One-Hot Encoding"
   ],
   "id": "88758720bb63d408"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T17:22:41.432843Z",
     "start_time": "2025-12-02T17:22:41.425868Z"
    }
   },
   "source": [
    "def ohe_encoding(datasets_list):\n",
    "    \"\"\"\n",
    "    Применяет One-Hot Encoding к категориальным признакам датасетов.\n",
    "    Сохраняет результат в папку датасета и добавляет запись в data.csv.\n",
    "\n",
    "    Args:\n",
    "        datasets_list: список словарей с информацией о датасетах\n",
    "    \"\"\"\n",
    "    for dataset_info in datasets_list:\n",
    "        try:\n",
    "            print(f\"\\nОбработка {dataset_info['dataset_name']} - One-Hot Encoding...\")\n",
    "\n",
    "            # Загружаем датасет, корректируя путь\n",
    "            original_path = Path(dataset_info['dataset_path'])\n",
    "            df = pd.read_csv(Path('..') / original_path)\n",
    "            print(f\"  Загружено {len(df)} строк, {len(df.columns)} колонок\")\n",
    "\n",
    "            # Получаем категориальные колонки (исключая target, если он в списке)\n",
    "            cat_cols = [col for col in dataset_info['cat_cols'] if col in df.columns]\n",
    "            target = dataset_info['target']\n",
    "\n",
    "            # Удаляем target из cat_cols если он там есть\n",
    "            cat_cols_to_encode = [col for col in cat_cols if col != target]\n",
    "\n",
    "            # Применяем One-Hot Encoding с типом int\n",
    "            df_encoded = pd.get_dummies(df, columns=cat_cols_to_encode, prefix_sep='_', drop_first=False, dtype=int)\n",
    "\n",
    "            # Всегда применяем Label Encoding к целевой колонке\n",
    "            if target in df_encoded.columns:\n",
    "                le = LabelEncoder()\n",
    "                df_encoded[target] = le.fit_transform(df_encoded[target].astype(str))\n",
    "\n",
    "            # Получаем список новых категориальных колонок\n",
    "            new_cat_cols = [col for col in df_encoded.columns if col not in df.columns or col in cat_cols_to_encode]\n",
    "            # Добавляем target к новым колонкам если он категориальный\n",
    "            if target in dataset_info['cat_cols']:\n",
    "                new_cat_cols.append(target)\n",
    "\n",
    "            # Создаем папку для датасета, корректируя путь\n",
    "            dataset_folder = Path('..') / Path(dataset_info['dataset_csv']).parent\n",
    "            dataset_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Сохраняем закодированный датасет\n",
    "            output_path = dataset_folder / f\"{dataset_info['dataset_name']}_ohe.csv\"\n",
    "            df_encoded.to_csv(output_path, index=False)\n",
    "            print(f\"  Сохранено в {output_path}\")\n",
    "            print(f\"  Новый размер: {len(df_encoded)} строк, {len(df_encoded.columns)} колонок\")\n",
    "\n",
    "            # Обновляем запись в data.csv\n",
    "            data_csv_path = Path('..') / dataset_info['dataset_csv']\n",
    "            method_name = 'one_hot_encoding'\n",
    "\n",
    "            if os.path.exists(data_csv_path):\n",
    "                data_df = pd.read_csv(data_csv_path)\n",
    "                # Удаляем старую запись для этого метода, если она существует\n",
    "                data_df = data_df[data_df['method'] != method_name]\n",
    "            else:\n",
    "                data_df = pd.DataFrame(columns=['method', 'path', 'New_cat_cols', 'model_path', 'schedul_path',\n",
    "                                                 'JS divergence', 'Utility_XGBoost', 'Utility_logistic_r', 'matrix_distance'])\n",
    "\n",
    "            # Добавляем новую запись\n",
    "            new_row = {\n",
    "                'method': method_name,\n",
    "                'path': str(output_path.relative_to(Path('..'))), # Сохраняем относительный путь\n",
    "                'New_cat_cols': str(new_cat_cols),\n",
    "                'model_path': '',\n",
    "                'schedul_path': '',\n",
    "                'JS divergence': '',\n",
    "                'Utility_XGBoost': '',\n",
    "                'Utility_logistic_r': '',\n",
    "                'matrix_distance': ''\n",
    "            }\n",
    "            data_df = pd.concat([data_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "            data_df.to_csv(data_csv_path, index=False)\n",
    "            print(f\"  Запись для '{method_name}' обновлена в {data_csv_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Ошибка при обработке {dataset_info['dataset_name']}: {e}\")\n"
   ],
   "id": "1fc3fcfbc1b5c3db",
   "outputs": [],
   "execution_count": 215
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Функция Label Encoding"
   ],
   "id": "321549f4348f0f19"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T17:22:41.441091Z",
     "start_time": "2025-12-02T17:22:41.435808Z"
    }
   },
   "source": [
    "def label_encoding(datasets_list):\n",
    "    \"\"\"\n",
    "    Применяет Label Encoding к категориальным признакам датасетов.\n",
    "    Сохраняет результат в папку датасета и добавляет запись в data.csv.\n",
    "\n",
    "    Args:\n",
    "        datasets_list: список словарей с информацией о датасетах\n",
    "    \"\"\"\n",
    "    for dataset_info in datasets_list:\n",
    "        try:\n",
    "            print(f\"\\nОбработка {dataset_info['dataset_name']} - Label Encoding...\")\n",
    "\n",
    "            # Загружаем датасет, корректируя путь\n",
    "            original_path = Path(dataset_info['dataset_path'])\n",
    "            df = pd.read_csv(Path('..') / original_path)\n",
    "            print(f\"  Загружено {len(df)} строк, {len(df.columns)} колонок\")\n",
    "\n",
    "            # Получаем категориальные колонки\n",
    "            cat_cols = [col for col in dataset_info['cat_cols'] if col in df.columns]\n",
    "            target = dataset_info['target']\n",
    "\n",
    "            # Применяем Label Encoding\n",
    "            df_encoded = df.copy()\n",
    "            encoded_cols = []\n",
    "\n",
    "            for col in cat_cols:\n",
    "                le = LabelEncoder()\n",
    "                df_encoded[col] = le.fit_transform(df[col].astype(str))\n",
    "                encoded_cols.append(col)\n",
    "\n",
    "            # Всегда применяем Label Encoding к целевой колонке (если она еще не была обработана)\n",
    "            if target in df_encoded.columns and target not in encoded_cols:\n",
    "                le = LabelEncoder()\n",
    "                df_encoded[target] = le.fit_transform(df_encoded[target].astype(str))\n",
    "                encoded_cols.append(target)\n",
    "\n",
    "            # Формируем список новых категориальных колонок (все закодированные колонки)\n",
    "            new_cat_cols = encoded_cols\n",
    "\n",
    "            # Создаем папку для датасета, корректируя путь\n",
    "            dataset_folder = Path('..') / Path(dataset_info['dataset_csv']).parent\n",
    "            dataset_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Сохраняем закодированный датасет\n",
    "            output_path = dataset_folder / f\"{dataset_info['dataset_name']}_label.csv\"\n",
    "            df_encoded.to_csv(output_path, index=False)\n",
    "            print(f\"  Сохранено в {output_path}\")\n",
    "            print(f\"  Размер: {len(df_encoded)} строк, {len(df_encoded.columns)} колонок\")\n",
    "            print(f\"  Закодированные колонки: {new_cat_cols}\")\n",
    "\n",
    "            # Обновляем запись в data.csv\n",
    "            data_csv_path = Path('..') / dataset_info['dataset_csv']\n",
    "            method_name = 'label_encoding'\n",
    "\n",
    "            if os.path.exists(data_csv_path):\n",
    "                data_df = pd.read_csv(data_csv_path)\n",
    "                # Удаляем старую запись для этого метода, если она существует\n",
    "                data_df = data_df[data_df['method'] != method_name]\n",
    "            else:\n",
    "                data_df = pd.DataFrame(columns=['method', 'path', 'New_cat_cols', 'model_path', 'schedul_path',\n",
    "                                                 'JS divergence', 'Utility_XGBoost', 'Utility_logistic_r', 'matrix_distance'])\n",
    "\n",
    "            # Добавляем новую запись\n",
    "            new_row = {\n",
    "                'method': method_name,\n",
    "                'path': str(output_path.relative_to(Path('..'))), # Сохраняем относительный путь\n",
    "                'New_cat_cols': str(new_cat_cols),\n",
    "                'model_path': '',\n",
    "                'schedul_path': '',\n",
    "                'JS divergence': '',\n",
    "                'Utility_XGBoost': '',\n",
    "                'Utility_logistic_r': '',\n",
    "                'matrix_distance': ''\n",
    "            }\n",
    "            data_df = pd.concat([data_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "            data_df.to_csv(data_csv_path, index=False)\n",
    "            print(f\"  Запись для '{method_name}' обновлена в {data_csv_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Ошибка при обработке {dataset_info['dataset_name']}: {e}\")"
   ],
   "id": "b9fc952ba91b6e4d",
   "outputs": [],
   "execution_count": 216
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Функция Frequency Encoding"
   ],
   "id": "5dabb6105da7ad02"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T17:22:41.449188Z",
     "start_time": "2025-12-02T17:22:41.444167Z"
    }
   },
   "source": [
    "def frequency_encoding(datasets_list):\n",
    "    \"\"\"\n",
    "    Применяет Frequency Encoding к категориальным признакам датасетов.\n",
    "    Сохраняет результат в папку датасета и добавляет запись в data.csv.\n",
    "\n",
    "    Args:\n",
    "        datasets_list: список словарей с информацией о датасетах\n",
    "    \"\"\"\n",
    "    for dataset_info in datasets_list:\n",
    "        try:\n",
    "            print(f\"\\nОбработка {dataset_info['dataset_name']} - Frequency Encoding...\")\n",
    "\n",
    "            # Загружаем датасет, корректируя путь\n",
    "            original_path = Path(dataset_info['dataset_path'])\n",
    "            df = pd.read_csv(Path('..') / original_path)\n",
    "            print(f\"  Загружено {len(df)} строк, {len(df.columns)} колонок\")\n",
    "\n",
    "            # Получаем категориальные колонки\n",
    "            cat_cols = [col for col in dataset_info['cat_cols'] if col in df.columns]\n",
    "            target = dataset_info['target']\n",
    "\n",
    "            # Применяем Frequency Encoding (но не к target)\n",
    "            df_encoded = df.copy()\n",
    "            encoded_cols = []\n",
    "\n",
    "            for col in cat_cols:\n",
    "                # Пропускаем target, он будет обработан отдельно через LabelEncoder\n",
    "                if col == target:\n",
    "                    continue\n",
    "                # Вычисляем частоты\n",
    "                freq_map = df[col].value_counts(normalize=True).to_dict()\n",
    "                # Заменяем значения на частоты\n",
    "                df_encoded[col] = df[col].map(freq_map)\n",
    "                encoded_cols.append(col)\n",
    "\n",
    "            # Всегда применяем Label Encoding к целевой колонке\n",
    "            if target in df_encoded.columns:\n",
    "                le = LabelEncoder()\n",
    "                df_encoded[target] = le.fit_transform(df_encoded[target].astype(str))\n",
    "                encoded_cols.append(target)\n",
    "\n",
    "            # Формируем список новых категориальных колонок (все закодированные колонки)\n",
    "            new_cat_cols = encoded_cols\n",
    "\n",
    "            # Создаем папку для датасета, корректируя путь\n",
    "            dataset_folder = Path('..') / Path(dataset_info['dataset_csv']).parent\n",
    "            dataset_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Сохраняем закодированный датасет\n",
    "            output_path = dataset_folder / f\"{dataset_info['dataset_name']}_frequency.csv\"\n",
    "            df_encoded.to_csv(output_path, index=False)\n",
    "            print(f\"  Сохранено в {output_path}\")\n",
    "            print(f\"  Размер: {len(df_encoded)} строк, {len(df_encoded.columns)} колонок\")\n",
    "            print(f\"  Закодированные колонки: {new_cat_cols}\")\n",
    "\n",
    "            # Обновляем запись в data.csv\n",
    "            data_csv_path = Path('..') / dataset_info['dataset_csv']\n",
    "            method_name = 'frequency_encoding'\n",
    "\n",
    "            if os.path.exists(data_csv_path):\n",
    "                data_df = pd.read_csv(data_csv_path)\n",
    "                # Удаляем старую запись для этого метода, если она существует\n",
    "                data_df = data_df[data_df['method'] != method_name]\n",
    "            else:\n",
    "                data_df = pd.DataFrame(columns=['method', 'path', 'New_cat_cols', 'model_path', 'schedul_path',\n",
    "                                                 'JS divergence', 'Utility_XGBoost', 'Utility_logistic_r', 'matrix_distance'])\n",
    "\n",
    "            # Добавляем новую запись\n",
    "            new_row = {\n",
    "                'method': method_name,\n",
    "                'path': str(output_path.relative_to(Path('..'))), # Сохраняем относительный путь\n",
    "                'New_cat_cols': str(new_cat_cols),\n",
    "                'model_path': '',\n",
    "                'schedul_path': '',\n",
    "                'JS divergence': '',\n",
    "                'Utility_XGBoost': '',\n",
    "                'Utility_logistic_r': '',\n",
    "                'matrix_distance': ''\n",
    "            }\n",
    "            data_df = pd.concat([data_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "            data_df.to_csv(data_csv_path, index=False)\n",
    "            print(f\"  Запись для '{method_name}' обновлена в {data_csv_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Ошибка при обработке {dataset_info['dataset_name']}: {e}\")"
   ],
   "id": "b7f43243a5336522",
   "outputs": [],
   "execution_count": 217
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Функция Original (без кодирования, только target)",
   "id": "9bf1b8d97b44d090"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T17:22:41.456107Z",
     "start_time": "2025-12-02T17:22:41.451919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def original_encoding(datasets_list):\n",
    "    \"\"\"\n",
    "    Сохраняет оригинальный датасет с Label Encoding только для целевой колонки.\n",
    "    Все остальные колонки остаются без изменений.\n",
    "\n",
    "    Args:\n",
    "        datasets_list: список словарей с информацией о датасетах\n",
    "    \"\"\"\n",
    "    for dataset_info in datasets_list:\n",
    "        try:\n",
    "            print(f\"\\nОбработка {dataset_info['dataset_name']} - Original (только target закодирован)...\")\n",
    "\n",
    "            # Загружаем датасет, корректируя путь\n",
    "            original_path = Path(dataset_info['dataset_path'])\n",
    "            df = pd.read_csv(Path('..') / original_path)\n",
    "            print(f\"  Загружено {len(df)} строк, {len(df.columns)} колонок\")\n",
    "\n",
    "            target = dataset_info['target']\n",
    "\n",
    "            # Копируем датасет без изменений\n",
    "            df_encoded = df.copy()\n",
    "\n",
    "            # Применяем Label Encoding только к целевой колонке\n",
    "            if target in df_encoded.columns:\n",
    "                le = LabelEncoder()\n",
    "                df_encoded[target] = le.fit_transform(df_encoded[target].astype(str))\n",
    "                print(f\"  Целевая колонка '{target}' закодирована с помощью LabelEncoder\")\n",
    "\n",
    "            # Список категориальных колонок - только target (если он был категориальным)\n",
    "            new_cat_cols = [target] if target in dataset_info['cat_cols'] and target in df_encoded.columns else []\n",
    "\n",
    "            # Создаем папку для датасета, корректируя путь\n",
    "            dataset_folder = Path('..') / Path(dataset_info['dataset_csv']).parent\n",
    "            dataset_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Сохраняем датасет\n",
    "            output_path = dataset_folder / f\"{dataset_info['dataset_name']}_original.csv\"\n",
    "            df_encoded.to_csv(output_path, index=False)\n",
    "            print(f\"  Сохранено в {output_path}\")\n",
    "            print(f\"  Размер: {len(df_encoded)} строк, {len(df_encoded.columns)} колонок\")\n",
    "            print(f\"  Закодированные колонки: {new_cat_cols}\")\n",
    "\n",
    "            # Обновляем запись в data.csv\n",
    "            data_csv_path = Path('..') / dataset_info['dataset_csv']\n",
    "            method_name = 'original'\n",
    "\n",
    "            if os.path.exists(data_csv_path):\n",
    "                data_df = pd.read_csv(data_csv_path)\n",
    "                # Удаляем старую запись для этого метода, если она существует\n",
    "                data_df = data_df[data_df['method'] != method_name]\n",
    "            else:\n",
    "                data_df = pd.DataFrame(columns=['method', 'path', 'New_cat_cols', 'model_path', 'schedul_path',\n",
    "                                                 'JS divergence', 'Utility_XGBoost', 'Utility_logistic_r', 'matrix_distance'])\n",
    "\n",
    "            # Добавляем новую запись\n",
    "            new_row = {\n",
    "                'method': method_name,\n",
    "                'path': str(output_path.relative_to(Path('..'))), # Сохраняем относительный путь\n",
    "                'New_cat_cols': str(new_cat_cols),\n",
    "                'model_path': '',\n",
    "                'schedul_path': '',\n",
    "                'JS divergence': '',\n",
    "                'Utility_XGBoost': '',\n",
    "                'Utility_logistic_r': '',\n",
    "                'matrix_distance': ''\n",
    "            }\n",
    "            data_df = pd.concat([data_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "            data_df.to_csv(data_csv_path, index=False)\n",
    "            print(f\"  Запись для '{method_name}' обновлена в {data_csv_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Ошибка при обработке {dataset_info['dataset_name']}: {e}\")"
   ],
   "id": "613c03afa1dc01b7",
   "outputs": [],
   "execution_count": 218
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Запуск обработки датасетов",
   "id": "387723c50914ab8c"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T17:22:41.615606Z",
     "start_time": "2025-12-02T17:22:41.459058Z"
    }
   },
   "source": [
    "# Применяем One-Hot Encoding\n",
    "print(\"=\"*50)\n",
    "print(\"ONE-HOT ENCODING\")\n",
    "print(\"=\"*50)\n",
    "ohe_encoding(datasets)"
   ],
   "id": "2a7e47bb79048998",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ONE-HOT ENCODING\n",
      "==================================================\n",
      "\n",
      "Обработка adult - One-Hot Encoding...\n",
      "  Загружено 10000 строк, 15 колонок\n",
      "  Сохранено в ../datasets/adult/adult_ohe.csv\n",
      "  Новый размер: 10000 строк, 104 колонок\n",
      "  Запись для 'one_hot_encoding' обновлена в ../datasets/adult/data.csv\n",
      "\n",
      "Обработка magic_gamma - One-Hot Encoding...\n",
      "  Загружено 10000 строк, 11 колонок\n",
      "  Сохранено в ../datasets/magic_gamma/magic_gamma_ohe.csv\n",
      "  Новый размер: 10000 строк, 11 колонок\n",
      "  Запись для 'one_hot_encoding' обновлена в ../datasets/magic_gamma/data.csv\n"
     ]
    }
   ],
   "execution_count": 219
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T17:22:41.704211Z",
     "start_time": "2025-12-02T17:22:41.620721Z"
    }
   },
   "source": [
    "# Применяем Label Encoding\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LABEL ENCODING\")\n",
    "print(\"=\"*50)\n",
    "label_encoding(datasets)"
   ],
   "id": "743f3e609dde9812",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "LABEL ENCODING\n",
      "==================================================\n",
      "\n",
      "Обработка adult - Label Encoding...\n",
      "  Загружено 10000 строк, 15 колонок\n",
      "  Сохранено в ../datasets/adult/adult_label.csv\n",
      "  Размер: 10000 строк, 15 колонок\n",
      "  Закодированные колонки: ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country', 'class']\n",
      "  Запись для 'label_encoding' обновлена в ../datasets/adult/data.csv\n",
      "\n",
      "Обработка magic_gamma - Label Encoding...\n",
      "  Загружено 10000 строк, 11 колонок\n",
      "  Сохранено в ../datasets/magic_gamma/magic_gamma_label.csv\n",
      "  Размер: 10000 строк, 11 колонок\n",
      "  Закодированные колонки: ['class:']\n",
      "  Запись для 'label_encoding' обновлена в ../datasets/magic_gamma/data.csv\n"
     ]
    }
   ],
   "execution_count": 220
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T17:22:41.809462Z",
     "start_time": "2025-12-02T17:22:41.707836Z"
    }
   },
   "source": [
    "# Применяем Frequency Encoding\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FREQUENCY ENCODING\")\n",
    "print(\"=\"*50)\n",
    "frequency_encoding(datasets)"
   ],
   "id": "e98f97cb215056f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "FREQUENCY ENCODING\n",
      "==================================================\n",
      "\n",
      "Обработка adult - Frequency Encoding...\n",
      "  Загружено 10000 строк, 15 колонок\n",
      "  Сохранено в ../datasets/adult/adult_frequency.csv\n",
      "  Размер: 10000 строк, 15 колонок\n",
      "  Закодированные колонки: ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country', 'class']\n",
      "  Запись для 'frequency_encoding' обновлена в ../datasets/adult/data.csv\n",
      "\n",
      "Обработка magic_gamma - Frequency Encoding...\n",
      "  Загружено 10000 строк, 11 колонок\n",
      "  Сохранено в ../datasets/magic_gamma/magic_gamma_frequency.csv\n",
      "  Размер: 10000 строк, 11 колонок\n",
      "  Закодированные колонки: ['class:']\n",
      "  Запись для 'frequency_encoding' обновлена в ../datasets/magic_gamma/data.csv\n"
     ]
    }
   ],
   "execution_count": 221
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T17:22:41.901075Z",
     "start_time": "2025-12-02T17:22:41.816492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Применяем Original Encoding\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ORIGINAL ENCODING\")\n",
    "print(\"=\"*50)\n",
    "original_encoding(datasets)"
   ],
   "id": "1ef337dbc9cf7b52",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ORIGINAL ENCODING\n",
      "==================================================\n",
      "\n",
      "Обработка adult - Original (только target закодирован)...\n",
      "  Загружено 10000 строк, 15 колонок\n",
      "  Целевая колонка 'class' закодирована с помощью LabelEncoder\n",
      "  Сохранено в ../datasets/adult/adult_original.csv\n",
      "  Размер: 10000 строк, 15 колонок\n",
      "  Закодированные колонки: ['class']\n",
      "  Запись для 'original' обновлена в ../datasets/adult/data.csv\n",
      "\n",
      "Обработка magic_gamma - Original (только target закодирован)...\n",
      "  Загружено 10000 строк, 11 колонок\n",
      "  Целевая колонка 'class:' закодирована с помощью LabelEncoder\n",
      "  Сохранено в ../datasets/magic_gamma/magic_gamma_original.csv\n",
      "  Размер: 10000 строк, 11 колонок\n",
      "  Закодированные колонки: ['class:']\n",
      "  Запись для 'original' обновлена в ../datasets/magic_gamma/data.csv\n"
     ]
    }
   ],
   "execution_count": 222
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Проверка результатов",
   "id": "329282ec7ff710f9"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T17:22:41.907535Z",
     "start_time": "2025-12-02T17:22:41.904207Z"
    }
   },
   "source": [
    "# Просмотр обновленного data.csv для датасета adult\n",
    "data_csv_path = '../datasets/adult/data.csv'\n",
    "if os.path.exists(data_csv_path):\n",
    "    result_df = pd.read_csv(data_csv_path)\n",
    "    print(f\"Содержимое {data_csv_path}:\")\n",
    "    result_df.head()\n",
    "else:\n",
    "    print(f\"Файл {data_csv_path} не найден\")"
   ],
   "id": "705cad707d962e59",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Содержимое ../datasets/adult/data.csv:\n"
     ]
    }
   ],
   "execution_count": 223
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T17:22:41.919509Z",
     "start_time": "2025-12-02T17:22:41.913928Z"
    }
   },
   "cell_type": "code",
   "source": "result_df",
   "id": "5c2774c7aa66f750",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               method                                path  \\\n",
       "0    one_hot_encoding        datasets/adult/adult_ohe.csv   \n",
       "1      label_encoding      datasets/adult/adult_label.csv   \n",
       "2  frequency_encoding  datasets/adult/adult_frequency.csv   \n",
       "3            original   datasets/adult/adult_original.csv   \n",
       "\n",
       "                                        New_cat_cols  model_path  \\\n",
       "0  ['workclass_Federal-gov', 'workclass_Local-gov...         NaN   \n",
       "1  ['workclass', 'education', 'marital-status', '...         NaN   \n",
       "2  ['workclass', 'education', 'marital-status', '...         NaN   \n",
       "3                                          ['class']         NaN   \n",
       "\n",
       "   schedul_path  JS divergence  Utility_XGBoost  Utility_logistic_r  \\\n",
       "0           NaN            NaN              NaN                 NaN   \n",
       "1           NaN            NaN              NaN                 NaN   \n",
       "2           NaN            NaN              NaN                 NaN   \n",
       "3           NaN            NaN              NaN                 NaN   \n",
       "\n",
       "   matrix_distance  \n",
       "0              NaN  \n",
       "1              NaN  \n",
       "2              NaN  \n",
       "3              NaN  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>path</th>\n",
       "      <th>New_cat_cols</th>\n",
       "      <th>model_path</th>\n",
       "      <th>schedul_path</th>\n",
       "      <th>JS divergence</th>\n",
       "      <th>Utility_XGBoost</th>\n",
       "      <th>Utility_logistic_r</th>\n",
       "      <th>matrix_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one_hot_encoding</td>\n",
       "      <td>datasets/adult/adult_ohe.csv</td>\n",
       "      <td>['workclass_Federal-gov', 'workclass_Local-gov...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>label_encoding</td>\n",
       "      <td>datasets/adult/adult_label.csv</td>\n",
       "      <td>['workclass', 'education', 'marital-status', '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>frequency_encoding</td>\n",
       "      <td>datasets/adult/adult_frequency.csv</td>\n",
       "      <td>['workclass', 'education', 'marital-status', '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>original</td>\n",
       "      <td>datasets/adult/adult_original.csv</td>\n",
       "      <td>['class']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 224
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T17:22:41.947368Z",
     "start_time": "2025-12-02T17:22:41.943721Z"
    }
   },
   "source": [
    "# Сравнение размеров датасетов\n",
    "print(\"\\nСравнение размеров датасетов:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for dataset_info in datasets:\n",
    "    dataset_folder = Path(dataset_info['dataset_csv']).parent\n",
    "    dataset_name = dataset_info['dataset_name']\n",
    "    \n",
    "    print(f\"\\n{dataset_name.upper()}:\")\n",
    "    \n",
    "    # Оригинальный датасет\n",
    "    if os.path.exists(dataset_info['dataset_path']):\n",
    "        df_orig = pd.read_csv(dataset_info['dataset_path'])\n",
    "        print(f\"  Оригинал:    {df_orig.shape[0]:>6} строк × {df_orig.shape[1]:>3} колонок\")\n",
    "    \n",
    "    # OHE\n",
    "    ohe_path = dataset_folder / f\"{dataset_name}_ohe.csv\"\n",
    "    if os.path.exists(ohe_path):\n",
    "        df_ohe = pd.read_csv(ohe_path)\n",
    "        print(f\"  OHE:         {df_ohe.shape[0]:>6} строк × {df_ohe.shape[1]:>3} колонок\")\n",
    "    \n",
    "    # Label\n",
    "    label_path = dataset_folder / f\"{dataset_name}_label.csv\"\n",
    "    if os.path.exists(label_path):\n",
    "        df_label = pd.read_csv(label_path)\n",
    "        print(f\"  Label:       {df_label.shape[0]:>6} строк × {df_label.shape[1]:>3} колонок\")\n",
    "    \n",
    "    # Frequency\n",
    "    freq_path = dataset_folder / f\"{dataset_name}_frequency.csv\"\n",
    "    if os.path.exists(freq_path):\n",
    "        df_freq = pd.read_csv(freq_path)\n",
    "        print(f\"  Frequency:   {df_freq.shape[0]:>6} строк × {df_freq.shape[1]:>3} колонок\")\n",
    "\n",
    "    # Original\n",
    "    orig_path = dataset_folder / f\"{dataset_name}_original.csv\"\n",
    "    if os.path.exists(orig_path):\n",
    "        df_orig_enc = pd.read_csv(orig_path)\n",
    "        print(f\"  Original:    {df_orig_enc.shape[0]:>6} строк × {df_orig_enc.shape[1]:>3} колонок\")\n"
   ],
   "id": "46d38a592b5bb0f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Сравнение размеров датасетов:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ADULT:\n",
      "\n",
      "MAGIC_GAMMA:\n"
     ]
    }
   ],
   "execution_count": 225
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
