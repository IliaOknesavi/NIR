{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35122fd4993c3f18"
      },
      "source": [
        "# CTGAN finetuning с Optuna\n",
        "\n",
        "Нотбук подбирает гиперпараметры CTGAN для выбранного датасета и метода кодирования из реестра. В качестве целевой метрики используется качество модели, обученной на синтетике и проверенной на реальных данных (accuracy или R²). Лучшие параметры сохраняются в файл `<dataset_name>.txt` в папке `optuna_results`."
      ],
      "id": "35122fd4993c3f18"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-10T10:04:59.858578Z",
          "start_time": "2025-12-10T10:04:59.852230Z"
        },
        "id": "eb82553301381446"
      },
      "source": [
        "import ast\n",
        "import json\n",
        "from pathlib import Path\n",
        "import traceback\n",
        "import optuna\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ctgan import CTGAN\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from sklearn.metrics import accuracy_score, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# Конфигурация для полного запуска\n",
        "CANDIDATE_REGISTRY_PATHS = [\n",
        "    Path(\"datasets/datasets_registry.csv\"),\n",
        "    Path(\"../datasets/datasets_registry.csv\"),\n",
        "    Path(\"datasets_registry.csv\"),\n",
        "    Path(\"../datasets_registry.csv\"),\n",
        "]\n",
        "DATASETS_REGISTRY = next((p for p in CANDIDATE_REGISTRY_PATHS if p.exists()), CANDIDATE_REGISTRY_PATHS[0])\n",
        "dataset_name = \"adult\"                # имя датасета из реестра\n",
        "encoding_method = \"one_hot_encoding\"  # one_hot_encoding | label_encoding | frequency_encoding | original\n",
        "N_TRIALS = 120                         # количество попыток Optuna для полного поиска\n",
        "EPOCHS = 300                           # количество эпох обучения CTGAN на каждую попытку\n",
        "RANDOM_STATE = 42\n",
        "OUTPUT_DIR = Path(\"optuna_results\")\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "output_path = OUTPUT_DIR / f\"{dataset_name}.txt\"\n"
      ],
      "id": "eb82553301381446",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-10T10:04:59.872116Z",
          "start_time": "2025-12-10T10:04:59.864678Z"
        },
        "id": "dd9103cc7cf9238e"
      },
      "source": [
        "def load_registry(registry_path: Path = DATASETS_REGISTRY) -> pd.DataFrame:\n",
        "    if not registry_path.exists():\n",
        "        raise FileNotFoundError(\n",
        "            f\"Не найден datasets_registry.csv. Проверьте пути: {[str(p) for p in CANDIDATE_REGISTRY_PATHS]}\"\n",
        "        )\n",
        "    df = pd.read_csv(registry_path, skipinitialspace=True)\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "    df[\"dataset_name\"] = df[\"dataset_name\"].str.strip()\n",
        "    return df\n",
        "\n",
        "\n",
        "def _resolve_path(path_str: str, anchors: list[Path]) -> Path:\n",
        "    p = Path(path_str)\n",
        "    if p.is_absolute():\n",
        "        return p\n",
        "\n",
        "    # Если путь начинается с datasets/, пробуем от корня репозитория\n",
        "    if path_str.startswith(\"datasets/\"):\n",
        "        for anchor in anchors:\n",
        "            candidate = (anchor / path_str).resolve()\n",
        "            if candidate.exists():\n",
        "                return candidate\n",
        "\n",
        "    for anchor in anchors:\n",
        "        candidate = (anchor / path_str).resolve()\n",
        "        if candidate.exists():\n",
        "            return candidate\n",
        "\n",
        "    return (anchors[0] / path_str).resolve()\n",
        "\n",
        "\n",
        "def get_dataset_info(name: str) -> dict:\n",
        "    registry = load_registry()\n",
        "    row = registry.loc[registry[\"dataset_name\"] == name]\n",
        "    if row.empty:\n",
        "        raise ValueError(f\"Датасет {name} не найден в {DATASETS_REGISTRY}\")\n",
        "    rec = row.iloc[0]\n",
        "\n",
        "    repo_root = DATASETS_REGISTRY.parent.parent.resolve()\n",
        "    anchors = [repo_root, DATASETS_REGISTRY.parent.resolve(), Path.cwd()]\n",
        "\n",
        "    dataset_csv = _resolve_path(str(rec[\"dataset_csv\"]), anchors)\n",
        "    dataset_path = _resolve_path(str(rec[\"dataset_path\"]), anchors)\n",
        "\n",
        "    return {\n",
        "        \"dataset_csv\": dataset_csv,\n",
        "        \"dataset_path\": dataset_path,\n",
        "        \"target\": rec[\"target\"].strip(),\n",
        "    }\n",
        "\n",
        "\n",
        "def get_encoded_dataset(name: str, method: str):\n",
        "    info = get_dataset_info(name)\n",
        "    data_csv_path = info[\"dataset_csv\"]\n",
        "    if not data_csv_path.exists():\n",
        "        raise FileNotFoundError(\n",
        "            f\"{data_csv_path} не найден. Выполните notebooks/dataset_encoding.ipynb или скорректируйте путь в datasets_registry.csv\"\n",
        "        )\n",
        "\n",
        "    data_df = pd.read_csv(data_csv_path)\n",
        "    row = data_df.loc[data_df[\"method\"] == method]\n",
        "    if row.empty:\n",
        "        raise ValueError(f\"Метод {method} не найден в {data_csv_path}\")\n",
        "\n",
        "    rec = row.iloc[0]\n",
        "    new_cat_cols_raw = str(rec.get(\"New_cat_cols\", \"[]\"))\n",
        "    try:\n",
        "        new_cat_cols = ast.literal_eval(new_cat_cols_raw)\n",
        "    except Exception:\n",
        "        new_cat_cols = []\n",
        "\n",
        "    repo_root = DATASETS_REGISTRY.parent.parent.resolve()\n",
        "    anchors = [repo_root, DATASETS_REGISTRY.parent.resolve(), data_csv_path.parent.resolve(), Path.cwd()]\n",
        "    dataset_path = _resolve_path(str(rec[\"path\"]), anchors)\n",
        "\n",
        "    if not dataset_path.exists():\n",
        "        raise FileNotFoundError(\n",
        "            f\"{dataset_path} не найден. Перегенерируйте кодировки через dataset_encoding.ipynb\"\n",
        "        )\n",
        "\n",
        "    return {\n",
        "        \"csv_path\": dataset_path,\n",
        "        \"target\": info[\"target\"],\n",
        "        \"discrete_features\": [c for c in new_cat_cols if c],\n",
        "    }\n",
        "\n"
      ],
      "id": "dd9103cc7cf9238e",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-10T10:04:59.922525Z",
          "start_time": "2025-12-10T10:04:59.877727Z"
        },
        "id": "5f4adb179eed1ec5",
        "outputId": "1d628d35-bbad-4114-b519-3be2568e29fe"
      },
      "source": [
        "def prepare_data(csv_path: Path, target: str):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    if target not in df.columns:\n",
        "        raise ValueError(f\"Целевая колонка {target} отсутствует в {csv_path}\")\n",
        "    X = df.drop(columns=[target])\n",
        "    y = df[target]\n",
        "    is_regression = y.nunique() > 20 and pd.api.types.is_numeric_dtype(y)\n",
        "    return df, X, y, is_regression\n",
        "\n",
        "\n",
        "def evaluate_synthetic_quality(df_real: pd.DataFrame, df_synth: pd.DataFrame, target: str, is_regression: bool):\n",
        "    X_real = df_real.drop(columns=[target])\n",
        "    y_real = df_real[target]\n",
        "    X_synth = df_synth.drop(columns=[target])\n",
        "    y_synth = df_synth[target]\n",
        "\n",
        "    X_train_syn, _, y_train_syn, _ = train_test_split(\n",
        "        X_synth, y_synth, test_size=0.25, random_state=RANDOM_STATE, stratify=None\n",
        "    )\n",
        "    X_train_real, X_val_real, y_train_real, y_val_real = train_test_split(\n",
        "        X_real, y_real, test_size=0.25, random_state=RANDOM_STATE, stratify=None\n",
        "    )\n",
        "\n",
        "    if is_regression:\n",
        "        model = make_pipeline(StandardScaler(), Ridge(random_state=RANDOM_STATE))\n",
        "        model.fit(X_train_syn, y_train_syn)\n",
        "        preds = model.predict(X_val_real)\n",
        "        return r2_score(y_val_real, preds)\n",
        "\n",
        "    # Классификация\n",
        "    model = make_pipeline(\n",
        "        StandardScaler(),\n",
        "        LogisticRegression(max_iter=2000, solver=\"lbfgs\", n_jobs=4),\n",
        "    )\n",
        "    model.fit(X_train_syn, y_train_syn)\n",
        "    preds = model.predict(X_val_real)\n",
        "    return accuracy_score(y_val_real, preds)\n",
        "\n",
        "\n",
        "def objective(trial, df_real, target, discrete_features, is_regression):\n",
        "    params = {\n",
        "        \"embedding_dim\": trial.suggest_int(\"embedding_dim\", 64, 256, step=32),\n",
        "        \"gen_dim\": trial.suggest_categorical(\"gen_dim\", [128, 256, 512]),\n",
        "        \"disc_dim\": trial.suggest_categorical(\"disc_dim\", [64, 128, 256]),\n",
        "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [256, 512, 1024]),\n",
        "        \"generator_lr\": trial.suggest_float(\"generator_lr\", 1e-4, 5e-3, log=True),\n",
        "        \"discriminator_lr\": trial.suggest_float(\"discriminator_lr\", 1e-4, 5e-3, log=True),\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        ctgan = CTGAN(\n",
        "            epochs=EPOCHS,\n",
        "            batch_size=int(params[\"batch_size\"]),\n",
        "            embedding_dim=int(params[\"embedding_dim\"]),\n",
        "            generator_dim=(int(params[\"gen_dim\"]), int(params[\"gen_dim\"])),\n",
        "            discriminator_dim=(int(params[\"disc_dim\"]), int(params[\"disc_dim\"])),\n",
        "            generator_lr=params[\"generator_lr\"],\n",
        "            discriminator_lr=params[\"discriminator_lr\"],\n",
        "            pac=1,\n",
        "            enable_gpu=False,\n",
        "            verbose=False,\n",
        "        )\n",
        "\n",
        "        ctgan.fit(df_real, discrete_features)\n",
        "        df_synth = ctgan.sample(len(df_real))\n",
        "\n",
        "        score = evaluate_synthetic_quality(df_real, df_synth, target, is_regression)\n",
        "        return score\n",
        "    except Exception:\n",
        "        print(\"Trial failed:\")\n",
        "        traceback.print_exc()\n",
        "        return -np.inf\n",
        "\n",
        "\n",
        "encoded_info = get_encoded_dataset(dataset_name, encoding_method)\n",
        "df_real, X_real, y_real, is_regression = prepare_data(encoded_info[\"csv_path\"], encoded_info[\"target\"])\n",
        "discrete_features = [c for c in encoded_info[\"discrete_features\"] if c in df_real.columns]\n",
        "print(f\"Датасет: {dataset_name} | метод: {encoding_method} | строк: {len(df_real)} | фичей: {X_real.shape[1]} | дискретных: {len(discrete_features)}\")\n",
        "\n"
      ],
      "id": "5f4adb179eed1ec5",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Датасет: adult | метод: one_hot_encoding | строк: 10000 | фичей: 118 | дискретных: 113\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "is_executing": true
        },
        "ExecuteTime": {
          "start_time": "2025-12-10T10:04:59.928119Z"
        },
        "id": "e916daae8de2f54e",
        "outputId": "0412ca38-019f-4297-9155-9cbbb7d9adbd"
      },
      "source": [
        "pruner = optuna.pruners.MedianPruner(n_startup_trials=10, n_warmup_steps=0)\n",
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",\n",
        "    study_name=f\"ctgan_{dataset_name}_{encoding_method}\",\n",
        "    pruner=pruner,\n",
        ")\n",
        "study.optimize(\n",
        "    lambda trial: objective(trial, df_real, encoded_info[\"target\"], discrete_features, is_regression),\n",
        "    n_trials=N_TRIALS,\n",
        "    show_progress_bar=True,\n",
        ")\n",
        "\n",
        "print(\"Лучший результат:\", study.best_value)\n",
        "print(\"Параметры:\")\n",
        "print(json.dumps(study.best_trial.params, indent=2))\n",
        "\n",
        "result_payload = {\n",
        "    \"dataset\": dataset_name,\n",
        "    \"method\": encoding_method,\n",
        "    \"score\": study.best_value,\n",
        "    \"epochs\": EPOCHS,\n",
        "    \"n_trials\": N_TRIALS,\n",
        "    \"params\": study.best_trial.params,\n",
        "}\n",
        "\n",
        "output_path.write_text(json.dumps(result_payload, indent=2), encoding=\"utf-8\")\n",
        "print(f\"Сохранено: {output_path.resolve()}\")\n",
        "\n"
      ],
      "id": "e916daae8de2f54e",
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-12-10 13:04:59,929] A new study created in memory with name: ctgan_adult_one_hot_encoding\n",
            "  0%|          | 0/120 [00:00<?, ?it/s]"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## COLAB"
      ],
      "metadata": {
        "id": "bqsrG97ze8oE"
      },
      "id": "bqsrG97ze8oE"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec2eaed5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e377ff24-f99f-4a9b-afd0-2902f50e485c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "id": "ec2eaed5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84d6e3c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04bce619-1dbf-42a8-93e5-5f3ebd32db09"
      },
      "source": [
        "import os\n",
        "!git clone https://github.com/IliaOknesavi/NIR.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NIR'...\n",
            "remote: Enumerating objects: 284, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 284 (delta 0), reused 8 (delta 0), pack-reused 268 (from 1)\u001b[K\n",
            "Receiving objects: 100% (284/284), 108.07 MiB | 21.61 MiB/s, done.\n",
            "Resolving deltas: 100% (56/56), done.\n",
            "Updating files: 100% (142/142), done.\n"
          ]
        }
      ],
      "id": "84d6e3c4"
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/NIR\")"
      ],
      "metadata": {
        "id": "7v6s7Z1-6RiM"
      },
      "execution_count": 3,
      "outputs": [],
      "id": "7v6s7Z1-6RiM"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "whPR9GunDXVg",
        "outputId": "f9eb2065-ba6b-4ba9-d6be-00b622ee8d63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sdv>=1.10.0 (from -r requirements.txt (line 1))\n",
            "  Downloading sdv-1.30.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting sdmetrics>=0.13.1 (from -r requirements.txt (line 2))\n",
            "  Downloading sdmetrics-0.24.0-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting ctgan>=0.10.0 (from -r requirements.txt (line 3))\n",
            "  Downloading ctgan-0.11.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting openml>=0.14.1 (from -r requirements.txt (line 4))\n",
            "  Downloading openml-0.15.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas>=2.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn>=1.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (1.6.1)\n",
            "Requirement already satisfied: matplotlib>=3.7 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (3.10.0)\n",
            "Requirement already satisfied: tqdm>=4.66 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (4.67.1)\n",
            "Collecting category_encoders>=2.6.0 (from -r requirements.txt (line 10))\n",
            "  Downloading category_encoders-2.9.0-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: xgboost>=1.7 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (3.1.2)\n",
            "Collecting optuna>=3.5 (from -r requirements.txt (line 12))\n",
            "  Downloading optuna-4.6.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: scipy>=1.11 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 13)) (1.16.3)\n",
            "Collecting boto3<2.0.0,>=1.28 (from sdv>=1.10.0->-r requirements.txt (line 1))\n",
            "  Downloading boto3-1.42.6-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting botocore<2.0.0,>=1.31 (from sdv>=1.10.0->-r requirements.txt (line 1))\n",
            "  Downloading botocore-1.42.6-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: cloudpickle>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from sdv>=1.10.0->-r requirements.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: graphviz>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from sdv>=1.10.0->-r requirements.txt (line 1)) (0.21)\n",
            "Collecting copulas>=0.12.1 (from sdv>=1.10.0->-r requirements.txt (line 1))\n",
            "  Downloading copulas-0.12.3-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting deepecho>=0.7.0 (from sdv>=1.10.0->-r requirements.txt (line 1))\n",
            "  Downloading deepecho-0.7.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting rdt>=1.18.2 (from sdv>=1.10.0->-r requirements.txt (line 1))\n",
            "  Downloading rdt-1.18.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: platformdirs>=4.0 in /usr/local/lib/python3.12/dist-packages (from sdv>=1.10.0->-r requirements.txt (line 1)) (4.5.1)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.12/dist-packages (from sdv>=1.10.0->-r requirements.txt (line 1)) (6.0.3)\n",
            "Requirement already satisfied: plotly>=5.19.0 in /usr/local/lib/python3.12/dist-packages (from sdmetrics>=0.13.1->-r requirements.txt (line 2)) (5.24.1)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from ctgan>=0.10.0->-r requirements.txt (line 3)) (2.9.0+cu126)\n",
            "Collecting liac-arff>=2.4.0 (from openml>=0.14.1->-r requirements.txt (line 4))\n",
            "  Downloading liac-arff-2.5.0.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting xmltodict (from openml>=0.14.1->-r requirements.txt (line 4))\n",
            "  Downloading xmltodict-1.0.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from openml>=0.14.1->-r requirements.txt (line 4)) (2.32.4)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from openml>=0.14.1->-r requirements.txt (line 4)) (2.9.0.post0)\n",
            "Collecting minio (from openml>=0.14.1->-r requirements.txt (line 4))\n",
            "  Downloading minio-7.2.20-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (from openml>=0.14.1->-r requirements.txt (line 4)) (18.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from openml>=0.14.1->-r requirements.txt (line 4)) (25.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0->-r requirements.txt (line 5)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0->-r requirements.txt (line 5)) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.3->-r requirements.txt (line 7)) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.3->-r requirements.txt (line 7)) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->-r requirements.txt (line 8)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->-r requirements.txt (line 8)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->-r requirements.txt (line 8)) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->-r requirements.txt (line 8)) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->-r requirements.txt (line 8)) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->-r requirements.txt (line 8)) (3.2.5)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from category_encoders>=2.6.0->-r requirements.txt (line 10)) (1.0.2)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders>=2.6.0->-r requirements.txt (line 10)) (0.14.5)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost>=1.7->-r requirements.txt (line 11)) (2.27.5)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna>=3.5->-r requirements.txt (line 12)) (1.17.2)\n",
            "Collecting colorlog (from optuna>=3.5->-r requirements.txt (line 12))\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna>=3.5->-r requirements.txt (line 12)) (2.0.44)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna>=3.5->-r requirements.txt (line 12)) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna>=3.5->-r requirements.txt (line 12)) (4.15.0)\n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/jmespath/\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.28->sdv>=1.10.0->-r requirements.txt (line 1))\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.17.0,>=0.16.0 (from boto3<2.0.0,>=1.28->sdv>=1.10.0->-r requirements.txt (line 1))\n",
            "  Downloading s3transfer-0.16.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/dist-packages (from botocore<2.0.0,>=1.31->sdv>=1.10.0->-r requirements.txt (line 1)) (2.5.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly>=5.19.0->sdmetrics>=0.13.1->-r requirements.txt (line 2)) (9.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil->openml>=0.14.1->-r requirements.txt (line 4)) (1.17.0)\n",
            "Collecting Faker!=37.11.0,>=17 (from rdt>=1.18.2->sdv>=1.10.0->-r requirements.txt (line 1))\n",
            "  Downloading faker-38.2.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna>=3.5->-r requirements.txt (line 12)) (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.10.0->-r requirements.txt (line 3)) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.10.0->-r requirements.txt (line 3)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.10.0->-r requirements.txt (line 3)) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.10.0->-r requirements.txt (line 3)) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.10.0->-r requirements.txt (line 3)) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.10.0->-r requirements.txt (line 3)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.10.0->-r requirements.txt (line 3)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.10.0->-r requirements.txt (line 3)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.10.0->-r requirements.txt (line 3)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.10.0->-r requirements.txt (line 3)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.10.0->-r requirements.txt (line 3)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.10.0->-r requirements.txt (line 3)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.10.0->-r requirements.txt (line 3)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.10.0->-r requirements.txt (line 3)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.10.0->-r requirements.txt (line 3)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.10.0->-r requirements.txt (line 3)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.10.0->-r requirements.txt (line 3)) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.10.0->-r requirements.txt (line 3)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.10.0->-r requirements.txt (line 3)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.10.0->-r requirements.txt (line 3)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.10.0->-r requirements.txt (line 3)) (3.5.0)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.12/dist-packages (from minio->openml>=0.14.1->-r requirements.txt (line 4)) (25.1.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from minio->openml>=0.14.1->-r requirements.txt (line 4)) (2025.11.12)\n",
            "Collecting pycryptodome (from minio->openml>=0.14.1->-r requirements.txt (line 4))\n",
            "  Downloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->openml>=0.14.1->-r requirements.txt (line 4)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->openml>=0.14.1->-r requirements.txt (line 4)) (3.11)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.3.0->ctgan>=0.10.0->-r requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.12/dist-packages (from argon2-cffi->minio->openml>=0.14.1->-r requirements.txt (line 4)) (25.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.3.0->ctgan>=0.10.0->-r requirements.txt (line 3)) (3.0.3)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from argon2-cffi-bindings->argon2-cffi->minio->openml>=0.14.1->-r requirements.txt (line 4)) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->minio->openml>=0.14.1->-r requirements.txt (line 4)) (2.23)\n",
            "Downloading sdv-1.30.0-py3-none-any.whl (197 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.3/197.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sdmetrics-0.24.0-py3-none-any.whl (198 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.3/198.3 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ctgan-0.11.1-py3-none-any.whl (25 kB)\n",
            "Downloading openml-0.15.1-py3-none-any.whl (160 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.4/160.4 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading category_encoders-2.9.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.9/85.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optuna-4.6.0-py3-none-any.whl (404 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.42.6-py3-none-any.whl (140 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.42.6-py3-none-any.whl (14.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m125.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading copulas-0.12.3-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deepecho-0.7.0-py3-none-any.whl (27 kB)\n",
            "Downloading rdt-1.18.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Downloading minio-7.2.20-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.8/93.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xmltodict-1.0.2-py3-none-any.whl (13 kB)\n",
            "Downloading faker-38.2.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.16.0-py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: liac-arff\n",
            "  Building wheel for liac-arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for liac-arff: filename=liac_arff-2.5.0-py3-none-any.whl size=11717 sha256=a06ad17115b206a47685449cc71ac1e740c565d7c68ac9141223e124a3849678\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/ac/cf/c2919807a5c623926d217c0a18eb5b457e5c19d242c3b5963a\n",
            "Successfully built liac-arff\n",
            "Installing collected packages: xmltodict, pycryptodome, liac-arff, jmespath, Faker, colorlog, botocore, s3transfer, rdt, optuna, copulas, sdmetrics, minio, deepecho, ctgan, category_encoders, boto3, sdv, openml\n",
            "Successfully installed Faker-38.2.0 boto3-1.42.6 botocore-1.42.6 category_encoders-2.9.0 colorlog-6.10.1 copulas-0.12.3 ctgan-0.11.1 deepecho-0.7.0 jmespath-1.0.1 liac-arff-2.5.0 minio-7.2.20 openml-0.15.1 optuna-4.6.0 pycryptodome-3.23.0 rdt-1.18.2 s3transfer-0.16.0 sdmetrics-0.24.0 sdv-1.30.0 xmltodict-1.0.2\n"
          ]
        }
      ],
      "id": "whPR9GunDXVg"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}