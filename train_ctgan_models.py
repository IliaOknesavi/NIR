"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ CTGAN –º–æ–¥–µ–ª–µ–π –¥–ª—è –≤—Å–µ—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö –∏–∑ datasets_registry.csv –∏ data.csv.
"""

import pandas as pd
import numpy as np
from ctgan import CTGAN
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
from pathlib import Path
import ast
import os
import argparse


def load_datasets_registry(registry_path='datasets/datasets_registry.csv'):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ä–µ–µ—Å—Ç—Ä –¥–∞—Ç–∞—Å–µ—Ç–æ–≤."""
    df = pd.read_csv(registry_path, skipinitialspace=True)

    datasets_list = []
    for _, row in df.iterrows():
        if pd.isna(row['cat_col']):
            cat_cols_list = []
        else:
            cat_col_str = str(row['cat_col']).replace('\n', '').replace('\r', '').strip()
            try:
                if cat_col_str:
                    cat_cols_list = ast.literal_eval(cat_col_str)
                else:
                    cat_cols_list = []
            except (ValueError, SyntaxError):
                cat_cols_list = []

        dataset_info = {
            'dataset_name': row['dataset_name'].strip(),
            'dataset_path': row['dataset_path'].strip(),
            'dataset_csv': row['dataset_csv'].strip(),
            'target': row['target'].strip(),
            'cat_cols': cat_cols_list
        }
        datasets_list.append(dataset_info)

    return datasets_list


def load_encoded_datasets(dataset_info):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ—Ä—Å–∏—è—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞."""
    data_csv_path = Path(dataset_info['dataset_csv'])

    if not os.path.exists(data_csv_path):
        print(f"  ‚ö†Ô∏è –§–∞–π–ª {data_csv_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return []

    df = pd.read_csv(data_csv_path)

    encoded_datasets = []
    for _, row in df.iterrows():
        try:
            new_cat_cols = ast.literal_eval(str(row['New_cat_cols']))
        except:
            new_cat_cols = []

        encoded_info = {
            'method': row['method'],
            'path': row['path'],
            'New_cat_cols': new_cat_cols,
            'model_path': row.get('model_path', ''),
            'schedul_path': row.get('schedul_path', ''),
            'dataset_name': dataset_info['dataset_name'],
            'dataset_folder': Path(dataset_info['dataset_csv']).parent
        }
        encoded_datasets.append(encoded_info)

    return encoded_datasets


def plot_ctgan_losses(loss_df, smooth_window=10, save_path=None):
    """–°–æ–∑–¥–∞–µ—Ç –≥—Ä–∞—Ñ–∏–∫ –ª–æ—Å—Å–æ–≤ CTGAN."""
    if loss_df is None or len(loss_df) == 0:
        raise ValueError("loss_df –ø—É—Å—Ç–æ–π")

    cols_lower = {c.lower(): c for c in loss_df.columns}
    g_col = next((cols_lower[c] for c in cols_lower if "gen" in c), None)
    d_col = next((cols_lower[c] for c in cols_lower if "disc" in c), None)

    if g_col is None or d_col is None:
        if len(loss_df.columns) < 2:
            raise ValueError("loss_df –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ö–æ—Ç—è –±—ã 2 —Å—Ç–æ–ª–±—Ü–∞")
        g_col, d_col = loss_df.columns[:2]

    epochs = np.arange(1, len(loss_df) + 1)

    df = loss_df.copy()
    for c in [g_col, d_col]:
        df[c] = pd.to_numeric(df[c], errors="coerce")

    g_smooth = df[g_col].rolling(smooth_window, min_periods=1).mean()
    d_smooth = df[d_col].rolling(smooth_window, min_periods=1).mean()

    plt.figure(figsize=(10, 6), dpi=120)

    plt.plot(epochs, df[g_col], alpha=0.25, linewidth=1, label=f"{g_col} (raw)")
    plt.plot(epochs, df[d_col], alpha=0.25, linewidth=1, label=f"{d_col} (raw)")

    plt.plot(epochs, g_smooth, linewidth=2.5, label=f"{g_col} (smoothed)")
    plt.plot(epochs, d_smooth, linewidth=2.5, label=f"{d_col} (smoothed)")

    def annotate_series(y, name):
        y_last = float(y.iloc[-1])
        y_min = float(y.min())
        x_min = int(y.idxmin()) + 1
        plt.scatter([len(y)], [y_last], s=30)
        plt.text(len(y), y_last, f"  last: {y_last:.3f}", va="center")
        plt.scatter([x_min], [y_min], s=30)
        plt.text(x_min, y_min, f"  min@{x_min}: {y_min:.3f}", va="center")

    annotate_series(g_smooth, g_col)
    annotate_series(d_smooth, d_col)

    plt.title("CTGAN Training Losses")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.grid(True, linestyle="--", alpha=0.4)
    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))
    plt.legend(loc="best", frameon=True)
    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, bbox_inches="tight")
    plt.close()


def train_ctgan_for_encoded_dataset(encoded_info, epochs=300, verbose=True):
    """–¢—Ä–µ–Ω–∏—Ä—É–µ—Ç CTGAN –º–æ–¥–µ–ª—å."""
    try:
        data_path = Path(encoded_info['path'])
        print(f"\n{'='*70}")
        print(f"üìä –î–∞—Ç–∞—Å–µ—Ç: {encoded_info['dataset_name']}")
        print(f"üîß –ú–µ—Ç–æ–¥: {encoded_info['method']}")
        print(f"üìÅ –ü—É—Ç—å: {data_path}")

        if not os.path.exists(data_path):
            print(f"  ‚ö†Ô∏è –§–∞–π–ª {data_path} –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            return None, None

        df = pd.read_csv(data_path)
        print(f"  ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫, {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫")

        discrete_features = [col for col in encoded_info['New_cat_cols'] if col in df.columns]
        print(f"  üè∑Ô∏è –î–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(discrete_features)}")

        print(f"  üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ CTGAN ({epochs} —ç–ø–æ—Ö)...")
        ctgan = CTGAN(epochs=epochs, verbose=verbose)
        ctgan.fit(df, discrete_features)

        loss_df = ctgan.loss_values
        print(f"  ‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

        return ctgan, loss_df

    except Exception as e:
        print(f"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
        import traceback
        traceback.print_exc()
        return None, None


def save_ctgan_results(ctgan, loss_df, encoded_info, data_csv_path):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å, –≥—Ä–∞—Ñ–∏–∫ –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç data.csv."""
    try:
        dataset_folder = encoded_info['dataset_folder']
        dataset_name = encoded_info['dataset_name']
        method = encoded_info['method']

        models_folder = dataset_folder / 'models'
        schedules_folder = dataset_folder / 'training_schedules'
        models_folder.mkdir(exist_ok=True)
        schedules_folder.mkdir(exist_ok=True)

        model_filename = f"ctgan_{dataset_name}_{method}_model.pkl"
        schedule_filename = f"ctgan_{dataset_name}_{method}_losses.png"

        model_path = models_folder / model_filename
        schedule_path = schedules_folder / schedule_filename

        ctgan.save(str(model_path))
        print(f"  üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")

        plot_ctgan_losses(loss_df, save_path=str(schedule_path))
        print(f"  üìà –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {schedule_path}")

        df = pd.read_csv(data_csv_path)
        mask = df['method'] == method

        df.loc[mask, 'model_path'] = str(model_path)
        df.loc[mask, 'schedul_path'] = str(schedule_path)

        df.to_csv(data_csv_path, index=False)
        print(f"  üìù –û–±–Ω–æ–≤–ª–µ–Ω {data_csv_path}")

    except Exception as e:
        print(f"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏: {e}")
        import traceback
        traceback.print_exc()


def process_all_datasets(epochs=300, verbose=True, dataset_filter=None, method_filter=None):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –¥–∞—Ç–∞—Å–µ—Ç—ã –∏–∑ —Ä–µ–µ—Å—Ç—Ä–∞."""
    print("="*70)
    print("üöÄ CTGAN Model Creator")
    print("="*70)

    datasets = load_datasets_registry()
    print(f"\nüìã –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤: {len(datasets)}")

    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É
    if dataset_filter:
        datasets = [ds for ds in datasets if ds['dataset_name'] == dataset_filter]
        if not datasets:
            print(f"‚ùå –î–∞—Ç–∞—Å–µ—Ç '{dataset_filter}' –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return
        print(f"üîç –§–∏–ª—å—Ç—Ä: —Ç–æ–ª—å–∫–æ –¥–∞—Ç–∞—Å–µ—Ç '{dataset_filter}'")

    total_models = 0
    successful_models = 0

    for dataset_info in datasets:
        print(f"\n{'='*70}")
        print(f"üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞: {dataset_info['dataset_name']}")
        print(f"{'='*70}")

        encoded_datasets = load_encoded_datasets(dataset_info)
        print(f"  üìä –ù–∞–π–¥–µ–Ω–æ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ—Ä—Å–∏–π: {len(encoded_datasets)}")

        if len(encoded_datasets) == 0:
            print(f"  ‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç (–Ω–µ—Ç –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ—Ä—Å–∏–π)")
            continue

        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä –ø–æ –º–µ—Ç–æ–¥—É
        if method_filter:
            encoded_datasets = [enc for enc in encoded_datasets if enc['method'] == method_filter]
            if not encoded_datasets:
                print(f"  ‚ö†Ô∏è –ú–µ—Ç–æ–¥ '{method_filter}' –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                continue
            print(f"  üîç –§–∏–ª—å—Ç—Ä: —Ç–æ–ª—å–∫–æ –º–µ—Ç–æ–¥ '{method_filter}'")

        for encoded_info in encoded_datasets:
            total_models += 1

            ctgan, loss_df = train_ctgan_for_encoded_dataset(
                encoded_info,
                epochs=epochs,
                verbose=verbose
            )

            if ctgan is not None and loss_df is not None:
                data_csv_path = Path(dataset_info['dataset_csv'])
                save_ctgan_results(ctgan, loss_df, encoded_info, data_csv_path)
                successful_models += 1

    print(f"\n{'='*70}")
    print(f"‚úÖ –ó–ê–í–ï–†–®–ï–ù–û!")
    print(f"{'='*70}")
    print(f"  –í—Å–µ–≥–æ –º–æ–¥–µ–ª–µ–π: {total_models}")
    print(f"  –£—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–æ: {successful_models}")
    print(f"  –û—à–∏–±–æ–∫: {total_models - successful_models}")
    print(f"{'='*70}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='–¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ CTGAN –º–æ–¥–µ–ª–µ–π –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤')
    parser.add_argument('--epochs', type=int, default=300, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è')
    parser.add_argument('--dataset', type=str, default=None, help='–ò–º—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)')
    parser.add_argument('--method', type=str, default=None, help='–ú–µ—Ç–æ–¥ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)')
    parser.add_argument('--quiet', action='store_true', help='–ù–µ –≤—ã–≤–æ–¥–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è')

    args = parser.parse_args()

    process_all_datasets(
        epochs=args.epochs,
        verbose=not args.quiet,
        dataset_filter=args.dataset,
        method_filter=args.method
    )

