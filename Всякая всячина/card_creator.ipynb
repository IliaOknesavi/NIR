{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T17:28:15.605949Z",
     "start_time": "2025-11-18T17:28:15.601245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ctgan\n",
    "from ctgan import CTGAN"
   ],
   "id": "84a731f9c02a7098",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-18T17:28:15.622258Z",
     "start_time": "2025-11-18T17:28:15.609884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import dataclasses\n",
    "@dataclasses.dataclass\n",
    "class Card:\n",
    "    name: str\n",
    "    model: CTGAN\n",
    "    target: str\n",
    "    schedule_path: str\n",
    "    real_data: pd.DataFrame\n",
    "    synt_data: pd.DataFrame\n",
    "    jensen_shannon_divergence: pd.DataFrame\n",
    "    real_score: float\n",
    "    synt_score: float\n",
    "    def __init__(self, name: str, target: str, dataset_path: str, model_path: str, schedule_path: str):\n",
    "        self.name = name\n",
    "        self.target = target\n",
    "        self.model = ctgan.CTGAN.load(model_path)\n",
    "        self.real_data = pd.read_csv(dataset_path)\n",
    "        self.synt_data = self.model.sample(len(self.real_data))\n",
    "        self.schedule_path = schedule_path\n",
    "        self.jensen_shannon_divergence = pd.DataFrame()\n",
    "        self.real_score = 0.0\n",
    "        self.sint_score = 0.0\n"
   ],
   "id": "9bc281e0d15a660e",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T17:28:15.627875Z",
     "start_time": "2025-11-18T17:28:15.625121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def load_cards_from_params(params_dir: str) -> dict[str, Card]:\n",
    "    params_path = Path(params_dir)\n",
    "    cards: dict[str, Card] = {}\n",
    "    if not params_path.exists() or not params_path.is_dir():\n",
    "        return cards\n",
    "\n",
    "    for file in params_path.iterdir():\n",
    "        if not file.is_file():\n",
    "            continue\n",
    "\n",
    "        lines = [ln.strip() for ln in file.read_text(encoding='utf-8').splitlines() if ln.strip()]\n",
    "        if not lines:\n",
    "            continue\n",
    "        name = lines[0]\n",
    "        target = lines[1] if len(lines) > 1 else \"\"\n",
    "        dataset_path = lines[2] if len(lines) > 2 else \"\"\n",
    "        model_path = lines[3] if len(lines) > 3 else \"\"\n",
    "        schedule_path = lines[4] if len(lines) > 4 else \"\"\n",
    "        card = Card(name=name, target=target, dataset_path=dataset_path,\n",
    "                    model_path=model_path, schedule_path=schedule_path)\n",
    "        cards[name] = card\n",
    "\n",
    "    return cards"
   ],
   "id": "3b62ba415b449177",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T17:28:15.770630Z",
     "start_time": "2025-11-18T17:28:15.630239Z"
    }
   },
   "cell_type": "code",
   "source": "cards = load_cards_from_params('../models/params/')",
   "id": "f86066b63c0f0679",
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "Memo value not found at index 97",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mUnpicklingError\u001B[39m                           Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[85]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m cards = \u001B[43mload_cards_from_params\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m./models/params/\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[84]\u001B[39m\u001B[32m, line 21\u001B[39m, in \u001B[36mload_cards_from_params\u001B[39m\u001B[34m(params_dir)\u001B[39m\n\u001B[32m     19\u001B[39m     model_path = lines[\u001B[32m3\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(lines) > \u001B[32m3\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     20\u001B[39m     schedule_path = lines[\u001B[32m4\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(lines) > \u001B[32m4\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m21\u001B[39m     card = \u001B[43mCard\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m=\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdataset_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     22\u001B[39m \u001B[43m                \u001B[49m\u001B[43mmodel_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschedule_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mschedule_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     23\u001B[39m     cards[name] = card\n\u001B[32m     25\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m cards\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[83]\u001B[39m\u001B[32m, line 16\u001B[39m, in \u001B[36mCard.__init__\u001B[39m\u001B[34m(self, name, target, dataset_path, model_path, schedule_path)\u001B[39m\n\u001B[32m     14\u001B[39m \u001B[38;5;28mself\u001B[39m.name = name\n\u001B[32m     15\u001B[39m \u001B[38;5;28mself\u001B[39m.target = target\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m \u001B[38;5;28mself\u001B[39m.model = \u001B[43mctgan\u001B[49m\u001B[43m.\u001B[49m\u001B[43mCTGAN\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     17\u001B[39m \u001B[38;5;28mself\u001B[39m.real_data = pd.read_csv(dataset_path)\n\u001B[32m     18\u001B[39m \u001B[38;5;28mself\u001B[39m.synt_data = \u001B[38;5;28mself\u001B[39m.model.sample(\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m.real_data))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/JupyterProject/.venv/lib/python3.13/site-packages/ctgan/synthesizers/base.py:122\u001B[39m, in \u001B[36mBaseSynthesizer.load\u001B[39m\u001B[34m(cls, path)\u001B[39m\n\u001B[32m    120\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Load the model stored in the passed `path`.\"\"\"\u001B[39;00m\n\u001B[32m    121\u001B[39m device = torch.device(\u001B[33m'\u001B[39m\u001B[33mcuda:0\u001B[39m\u001B[33m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch.cuda.is_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[33m'\u001B[39m\u001B[33mcpu\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m122\u001B[39m model = \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweights_only\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m    123\u001B[39m model.set_device(device)\n\u001B[32m    124\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/JupyterProject/.venv/lib/python3.13/site-packages/torch/serialization.py:1554\u001B[39m, in \u001B[36mload\u001B[39m\u001B[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[39m\n\u001B[32m   1552\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m pickle.UnpicklingError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   1553\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m pickle.UnpicklingError(_get_wo_message(\u001B[38;5;28mstr\u001B[39m(e))) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1554\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_legacy_load\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1555\u001B[39m \u001B[43m    \u001B[49m\u001B[43mopened_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpickle_module\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mpickle_load_args\u001B[49m\n\u001B[32m   1556\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/JupyterProject/.venv/lib/python3.13/site-packages/torch/serialization.py:1802\u001B[39m, in \u001B[36m_legacy_load\u001B[39m\u001B[34m(f, map_location, pickle_module, **pickle_load_args)\u001B[39m\n\u001B[32m   1799\u001B[39m         \u001B[38;5;66;03m# if not a tarfile, reset file offset and proceed\u001B[39;00m\n\u001B[32m   1800\u001B[39m         f.seek(\u001B[32m0\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1802\u001B[39m magic_number = \u001B[43mpickle_module\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mpickle_load_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1803\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m magic_number != MAGIC_NUMBER:\n\u001B[32m   1804\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mInvalid magic number; corrupt file?\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mUnpicklingError\u001B[39m: Memo value not found at index 97"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T17:28:15.780375Z",
     "start_time": "2025-11-18T16:41:01.492618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Подавляем предупреждения для чистоты вывода\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "def evaluate_card(card: Card) -> None:\n",
    "    X_real = card.real_data.drop(columns=[card.target])\n",
    "    y_real = card.real_data[card.target]\n",
    "    X_synt = card.synt_data.drop(columns=[card.target])\n",
    "    y_synt = card.synt_data[card.target]\n",
    "\n",
    "    # Стратифицированный split для сохранения распределения классов\n",
    "    X_train_real, X_test_real, y_train_real, y_test_real = train_test_split(\n",
    "        X_real, y_real, test_size=0.5, random_state=42, stratify=y_real\n",
    "    )\n",
    "    X_train_synt, _, y_train_synt, _ = train_test_split(\n",
    "        X_synt, y_synt, test_size=0.5, random_state=42, stratify=y_synt\n",
    "    )\n",
    "\n",
    "    # Pipeline: StandardScaler + LogisticRegression без устаревшего параметра multi_class\n",
    "    clf_real = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        LogisticRegression(solver='saga', max_iter=20000, random_state=42)\n",
    "    )\n",
    "    clf_real.fit(X_train_real, y_train_real)\n",
    "    y_pred_real = clf_real.predict(X_test_real)\n",
    "    real_score = accuracy_score(y_test_real, y_pred_real)\n",
    "\n",
    "    clf_synt = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        LogisticRegression(solver='saga', max_iter=20000, random_state=42)\n",
    "    )\n",
    "    clf_synt.fit(X_train_synt, y_train_synt)\n",
    "    y_pred_synt = clf_synt.predict(X_test_real)\n",
    "    synt_score = accuracy_score(y_test_real, y_pred_synt)\n",
    "\n",
    "    card.real_score = real_score\n",
    "    card.synt_score = synt_score\n"
   ],
   "id": "92ebf47a0a205825",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T17:28:15.781437Z",
     "start_time": "2025-11-18T16:41:01.501480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_distributions(p_series: pd.Series, q_series: pd.Series):\n",
    "    \"\"\"\n",
    "    Вычисляет распределения вероятностей для двух серий (колонок) pandas.\n",
    "    Возвращает p, q на общем наборе уникальных значений.\n",
    "    \"\"\"\n",
    "    # 1. Получаем массив уникальных элементов из обеих колонок\n",
    "    all_values = pd.Index(p_series.unique()).union(q_series.unique())\n",
    "\n",
    "    # 2. Считаем вероятности для каждого уникального элемента\n",
    "    p_dist = p_series.value_counts(normalize=True).reindex(all_values, fill_value=0)\n",
    "    q_dist = q_series.value_counts(normalize=True).reindex(all_values, fill_value=0)\n",
    "\n",
    "    return p_dist, q_dist\n",
    "\n",
    "def calculate_metrics(p_df: pd.DataFrame, q_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Вычисляет энтропию, KL и JSD дивергенции для каждой колонки.\n",
    "    \"\"\"\n",
    "    metrics = []\n",
    "    epsilon = 1e-10  # Малая константа для избежания деления на ноль\n",
    "\n",
    "    for col in p_df.columns:\n",
    "        if col not in q_df.columns:\n",
    "            continue\n",
    "\n",
    "        p, q = calculate_distributions(p_df[col], q_df[col])\n",
    "\n",
    "        # --- Энтропия Шеннона для реальных данных H(P) ---\n",
    "        # Используем только ненулевые вероятности, так как 0*log(0) = 0\n",
    "        p_nonzero = p[p > 0]\n",
    "        shannon_entropy = -np.sum(p_nonzero * np.log2(p_nonzero))\n",
    "\n",
    "        # --- Дивергенция Кульбака-Лейблера D_KL(P || Q) ---\n",
    "        # Добавляем epsilon к q, чтобы избежать log(0) или деления на 0\n",
    "        q_smooth = q + epsilon\n",
    "        kl_divergence = np.sum(p_nonzero * np.log2(p_nonzero / q_smooth[p_nonzero.index]))\n",
    "\n",
    "        # --- Дивергенция Йенсена-Шеннона JSD(P || Q) ---\n",
    "        m = 0.5 * (p + q)\n",
    "        m_smooth = m + epsilon\n",
    "\n",
    "        # D_KL(P || M)\n",
    "        kl_p_m = np.sum(p_nonzero * np.log2(p_nonzero / m_smooth[p_nonzero.index]))\n",
    "\n",
    "        # D_KL(Q || M)\n",
    "        q_nonzero = q[q > 0]\n",
    "        kl_q_m = np.sum(q_nonzero * np.log2(q_nonzero / m_smooth[q_nonzero.index]))\n",
    "\n",
    "        jensen_shannon_divergence = 0.5 * kl_p_m + 0.5 * kl_q_m\n",
    "\n",
    "        metrics.append({\n",
    "            'column': col,\n",
    "            'shannon_entropy': shannon_entropy,\n",
    "            'kl_divergence': kl_divergence,\n",
    "            'jensen_shannon_divergence': jensen_shannon_divergence\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(metrics)[['column','jensen_shannon_divergence']]\n",
    "\n",
    "# Пример использования с вашим объектом Card\n",
    "# for card in cards.values():\n",
    "#     # Вычисляем метрики\n",
    "#     metrics_df = calculate_metrics(card.real_data, card.synt_data)\n",
    "#     # Сохраняем результат (например, в новый атрибут)\n",
    "#     card.metrics = metrics_df\n",
    "#     print(f\"Метрики для {card.name}:\")\n",
    "#     print(card.metrics)\n",
    "#     print(\"-\" * 30)\n"
   ],
   "id": "177575bcfcaa4db6",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T17:28:15.781874Z",
     "start_time": "2025-11-18T16:41:01.513753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for card in cards.values():\n",
    "    evaluate_card(card)\n",
    "    card.jensen_shannon_divergence = calculate_metrics(card.real_data, card.synt_data)"
   ],
   "id": "feafeb57acb68ed3",
   "outputs": [],
   "execution_count": 81
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
